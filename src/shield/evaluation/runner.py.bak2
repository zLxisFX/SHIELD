"""
Evaluation runner: compare robust + baselines on the same forcing/building.

Key goals:
- Make sure baselines are actually simulated with their own action schedules
  (not accidentally re-running the heuristic engine policy for everything).
- Make % vs ref robust when the reference metric is 0 (show "n/a" instead of
  weird sentinel values).
- Provide a JSON export that is always serializable (datetimes -> isoformat).
"""

from __future__ import annotations

import json
from dataclasses import asdict, is_dataclass
from datetime import datetime
from pathlib import Path
from typing import Any, Dict, List, Optional, Tuple

from shield.core.state import BuildingProfile, ForcingHour
from shield.engine.simulate import simulate_demo, _simulate_given_actions
from shield.evaluation.baselines import baseline_actions, list_baselines


def _pct_vs_ref(ref: float, val: float) -> Optional[float]:
    """
    Percent improvement vs reference, where positive means "better" (lower).

      pct = 100 * (ref - val) / ref

    If ref == 0:
      - if val == 0 => 0.0
      - else => None (not defined; caller should print "n/a")
    """
    if ref <= 0.0:
        if val == 0.0:
            return 0.0
        return None
    return 100.0 * (ref - val) / ref


def _fmt_pct(x: Optional[float], width: int = 11) -> str:
    if x is None:
        return f"{'n/a':>{width}}"
    return f"{x:+.1f}%".rjust(width)


def _json_default(o: Any) -> Any:
    # Make export robust: datetimes, Paths, dataclasses, simple objects.
    if isinstance(o, datetime):
        return o.isoformat()
    if isinstance(o, Path):
        return str(o)
    if is_dataclass(o):
        return asdict(o)
    if hasattr(o, "__dict__"):
        return o.__dict__
    return str(o)


def evaluate_policies(
    forcing: List[ForcingHour],
    building: BuildingProfile,
    *,
    robust_kwargs: Optional[Dict[str, Any]] = None,
    eval_ref: str = "heuristic",
    include_baselines: bool = True,
    pm25_threshold: float = 15.0,
    heat_threshold_c: float = 32.0,
    pm25_init: float = 8.0,
    temp_init_c: float = 28.5,
) -> Tuple[List[Dict[str, Any]], Dict[str, Any]]:
    """
    Returns:
      rows: list of dicts with summary + % vs ref fields
      results: mapping policy_name -> RunResult-like object (from engine)
    """
    results: Dict[str, Any] = {}

    # --- Robust (UQ-aware) ---
    rk = dict(robust_kwargs or {})
    # provide safe defaults if missing
    rk.setdefault("n_ensemble", 60)
    rk.setdefault("seed", 123)

    results["robust"] = simulate_demo(
        forcing=forcing,
        building=building,
        pm25_threshold=pm25_threshold,
        heat_threshold_c=heat_threshold_c,
        pm25_init=pm25_init,
        temp_init_c=temp_init_c,
        policy="robust_greedy",
        policy_kwargs=rk,
    )

    # --- Baselines (action schedules) ---
    if include_baselines:
        for name in list_baselines():
            acts = baseline_actions(forcing, building, name)
            results[name] = _simulate_given_actions(
                forcing=forcing,
                building=building,
                actions=acts,
                pm25_threshold=pm25_threshold,
                heat_threshold_c=heat_threshold_c,
                pm25_init=pm25_init,
                temp_init_c=temp_init_c,
            )

    # Ensure there is always a heuristic reference available
    if "heuristic" not in results:
        results["heuristic"] = simulate_demo(
            forcing=forcing,
            building=building,
            pm25_threshold=pm25_threshold,
            heat_threshold_c=heat_threshold_c,
            pm25_init=pm25_init,
            temp_init_c=temp_init_c,
            policy="heuristic",
            policy_kwargs=None,
        )

    # --- Reference policy ---
    ref_name = eval_ref if eval_ref in results else "heuristic"
    ref = results[ref_name]
    ref_pm = float(getattr(ref, "minutes_pm25_above_threshold"))
    ref_heat = float(getattr(ref, "hours_heat_above_threshold"))

    # --- Build rows in a stable order ---
    order: List[str] = ["robust"]
    if include_baselines:
        # keep list_baselines() order, but avoid duplicates if someone includes "robust"
        for n in list_baselines():
            if n != "robust":
                order.append(n)
    if "heuristic" not in order:
        order.append("heuristic")

    rows: List[Dict[str, Any]] = []
    for name in order:
        run = results[name]
        pm = int(getattr(run, "minutes_pm25_above_threshold"))
        heat = int(getattr(run, "hours_heat_above_threshold"))

        rows.append(
            {
                "policy": name,
                "pm_minutes": pm,
                "heat_hours": heat,
                "pm_pct_vs_ref": _pct_vs_ref(ref_pm, float(pm)),
                "heat_pct_vs_ref": _pct_vs_ref(ref_heat, float(heat)),
                "ref_policy": ref_name,
            }
        )

    return rows, results


def policy_table_string(rows: List[Dict[str, Any]], *, eval_ref: str = "heuristic") -> str:
    # Determine column widths
    policy_w = max(len("Policy"), max(len(str(r.get("policy", ""))) for r in rows)) if rows else len("Policy")

    header = (
        f"{'Policy'.ljust(policy_w)} | "
        f"{'PM minutes'.rjust(10)} | "
        f"{'Heat hours'.rjust(9)} | "
        f"{'PM % vs ref'.rjust(11)} | "
        f"{'Heat % vs ref'.rjust(12)}"
    )
    sep = (
        f"{'-' * policy_w}-+-"
        f"{'-' * 10}-+-"
        f"{'-' * 9}-+-"
        f"{'-' * 11}-+-"
        f"{'-' * 12}"
    )

    lines = [header, sep]
    for r in rows:
        lines.append(
            f"{str(r['policy']).ljust(policy_w)} | "
            f"{int(r['pm_minutes']):>10d} | "
            f"{int(r['heat_hours']):>9d} | "
            f"{_fmt_pct(r.get('pm_pct_vs_ref'), width=11)} | "
            f"{_fmt_pct(r.get('heat_pct_vs_ref'), width=12)}"
        )
    return "\n".join(lines)


def export_eval_json(
    rows: List[Dict[str, Any]],
    results: Dict[str, Any],
    *,
    out_dir: str | Path = "outputs",
    prefix: str = "shield_eval",
) -> Path:
    out_dir = Path(out_dir)
    out_dir.mkdir(parents=True, exist_ok=True)

    payload = {
        "generated_at": datetime.now().isoformat(timespec="seconds"),
        "rows": rows,
        "results": results,
    }

    out = out_dir / f"{prefix}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
    out.write_text(json.dumps(payload, indent=2, default=_json_default), encoding="utf-8")
    return out


if __name__ == "__main__":
    # Minimal smoke test when running:
    #   python -m shield.evaluation.runner
    from shield.demo.scenarios import make_demo_forcing
    from shield.core.state import BuildingProfile

    b = BuildingProfile(archetype="classroom", floor_area_m2=90.0, has_hepa=True, has_fan=True, occupants=25)
    forcing = make_demo_forcing(start=datetime(2026, 1, 15, 0, 0), horizon_hours=72, scenario="smoke_heat_day")

    rows, results = evaluate_policies(forcing, b, robust_kwargs={"n_ensemble": 60, "seed": 123}, eval_ref="heuristic")
    print(policy_table_string(rows, eval_ref="heuristic"))
